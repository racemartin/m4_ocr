
# TechNova Attrition Analysis - PrÃ©disez et comprenez le turnover des employÃ©s.

<div align="left">
  <img src="docs/images/logo_technova.png" width="200px" alt="Logo TechNova Partners">
</div>

## **Objectif**: Identifier les causes racines de l'attrition et prÃ©dire le dÃ©part des collaborateurs Ã  l'aide de **XGBoost** et **SHAP**.

## ğŸ› ï¸ Technologies

* **Python 3.12+**
* **uv** (Gestionnaire de paquets ultra-rapide)
* **Scikit-learn, XGBoost** (ModÃ©lisation)
* **SHAP** (InterprÃ©tabilitÃ© des modÃ¨les)
* **Pandas, Seaborn** (Analyse de donnÃ©es)

## ğŸ“¦ Installation

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/racemartin/m4_ocr.git
cd m4_ocr

# Installer les dÃ©pendances et crÃ©er l'environnement virtuel avec uv
uv sync

# VÃ©rifier l'installation
uv run python -c "import pandas, shap, xgboost; print('Environnement prÃªt !')"

```

## ğŸš€ Utilisation

```bash
# ExÃ©cuter Jupyter Lab via uv
uv run jupyter lab

# Lancer l'analyse exploratoire automatique
uv run python -m ydata_profiling --config src/data/config_eda.yaml

```

### Notebooks Importants (Ordre d'implÃ©mentation)

| Ã‰tape | Notebook | Focus | EntrÃ©e | Sortie |
| --- | --- | --- | --- | --- |
| **1. EDA** | `01_RC_EDA_Nettoyage.ipynb` | Fusion & Nettoyage | RAW CSV | `interim/data_merged.csv` |
| **2. FE** | `02_RC_Feature_Engineering.ipynb` | Encodage & Features | interim | `processed/data_final.csv` |
| **3. MOD** | `03_RC_Modelisation.ipynb` | EntraÃ®nement XGBoost | processed | `models/V1/attrition_v1.joblib` |
| **3. MOD** | `04_RC_Modelisation_V2.ipynb` | EntraÃ®nement XGBoost | processed | `models/V2/attrition_v1.pkl` |
| **4. SHAP** | `05_RC_Interpretation_SHAP.ipynb` | **Causes du Turnover** | model | Reports/Figures |

## ğŸ“ Structure du projet

```bash
.
â”œâ”€â”€ .venv               # Environnement virtuel (isolÃ© par uv)
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw             # Fichiers originaux (SIRH, Eval, Sondage)
â”‚   â”œâ”€â”€ interim         # DonnÃ©es aprÃ¨s fusion (.merge())
â”‚   â””â”€â”€ processed       # DonnÃ©es prÃªtes pour XGBoost
â”œâ”€â”€ models              # ModÃ¨les sÃ©rialisÃ©s (.pkl)
â”œâ”€â”€ notebooks           # ExpÃ©rimentations pas Ã  pas
â”œâ”€â”€ pyproject.toml      # Configuration des dÃ©pendances (format uv)
â”œâ”€â”€ src                 # Scripts Python modulaires
â””â”€â”€ reports             # RÃ©sultats SHAP et graphiques pour Amandine

```

Details 

```bash
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile           <- Makefile with commands like `make data` or `make train`
    â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external       <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw            <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ docs               <- A default Sphinx project; see sphinx-doc.org for details
    â”‚
    â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                         the creator's initials, and a short `-` delimited description, e.g.
    â”‚                         `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                         generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    â”œâ”€â”€ src                <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py    <- Makes src a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data           <- Scripts to download or generate data
    â”‚Â Â  â”‚Â Â  â””â”€â”€ make_dataset.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling
    â”‚Â Â  â”‚Â Â  â””â”€â”€ build_features.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models         <- Scripts to train models and then use trained models to make
    â”‚   â”‚   â”‚                 predictions
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ predict_model.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model.py
    â”‚   â”‚
    â”‚Â Â  â””â”€â”€ visualization  <- Scripts to create exploratory and results oriented visualizations
    â”‚Â Â      â””â”€â”€ visualize.py
    â”‚
    â””â”€â”€ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io

```


## ğŸ“š DÃ©pendances principales

### Production

* **pandas** : Manipulation des bases RH.
* **xgboost** : Algorithme de boosting pour la prÃ©diction d'attrition.
* **shap** : Calcul des valeurs de Shapley pour expliquer "pourquoi" un employÃ© part.
* **scikit-learn** : Pipelines de transformation et mÃ©triques d'Ã©valuation.

### DÃ©veloppement

* **ydata-profiling** : GÃ©nÃ©ration de rapports EDA rapides.
* **yellowbrick** : Visualisation de la performance des classifieurs (Matrice de confusion, ROC).
* **black / flake8** : Garantie d'un code propre et standardisÃ©.

## ğŸ‘¤ Auteur

**Rafael Cerezo MartÃ­n**

* Email: [rafael.cerezo.martin@icloud.com](mailto:rafael.cerezo.martin@icloud.com)
* GitHub: [@racemartin](https://github.com/racemartin)

## ğŸ“„ Licence

MIT License - voir le fichier [LICENSE](https://www.google.com/search?q=LICENSE) pour plus de dÃ©tails.

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
